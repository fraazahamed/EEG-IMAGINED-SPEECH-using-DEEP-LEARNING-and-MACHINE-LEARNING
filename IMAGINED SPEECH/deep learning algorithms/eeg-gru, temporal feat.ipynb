{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import os.path as op\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\sherl\\\\Downloads\\\\nit pendrive\\\\Deep-Learning-for-BCI-master\\\\Deep-Learning-for-BCI-master\\\\tutorial')\n",
    "import myimporter\n",
    "from BCI_functions import *  # BCI_functions.ipynb contains some functions we might use multiple times in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windowing(X,Y):\n",
    "    X_new = []\n",
    "    Y_new = []\n",
    "    npt = 128 # .5 sec\n",
    "    stride = 8 # around .1 sec\n",
    "    ctr = 0\n",
    "    for i in range(0,X.shape[0]):\n",
    "        y = Y[i]\n",
    "        a= X[i,:,:]\n",
    "        a = a.transpose()\n",
    "        a.shape\n",
    "        val = 0\n",
    "        kd=len(a)\n",
    "        while val<=(len(a)-npt):\n",
    "            x = a[val:val+npt,:]\n",
    "\n",
    "            X_new.append(x.T)\n",
    "            Y_new.append(y)\n",
    "            val = val+stride\n",
    "            \n",
    "    return np.array(X_new),np.array(Y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\sherl\\Downloads\\Vipin_Apple.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    }
   ],
   "source": [
    "# Load .edf file\n",
    "filename = \"C:\\\\Users\\\\sherl\\\\Downloads\\\\Vipin_Apple.edf\"\n",
    "raw = mne.io.read_raw_edf(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Measurement date</th>\n",
       "        \n",
       "        <td>February 01, 2017  12:10:40 GMT</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Experimenter</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "        <th>Participant</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Digitized points</th>\n",
       "        \n",
       "        <td>Not available</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>39 EEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>EOG channels</th>\n",
       "        <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ECG channels</th>\n",
       "        <td>Not available</td>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Sampling frequency</th>\n",
       "        <td>250.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Highpass</th>\n",
       "        <td>0.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Lowpass</th>\n",
       "        <td>64.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Filenames</th>\n",
       "        <td>Vipin_Apple.edf</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Duration</th>\n",
       "        <td>00:00:13 (HH:MM:SS)</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<RawEDF | Vipin_Apple.edf, 39 x 3250 (13.0 s), ~1.0 MB, data loaded>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.resample(250, npad=\"auto\")    # set sampling frequency to 256 points per second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-6 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Measurement date</th>\n",
       "        \n",
       "        <td>February 01, 2017  12:10:40 GMT</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Experimenter</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "        <th>Participant</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Digitized points</th>\n",
       "        \n",
       "        <td>Not available</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>39 EEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>EOG channels</th>\n",
       "        <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ECG channels</th>\n",
       "        <td>Not available</td>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Sampling frequency</th>\n",
       "        <td>250.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Highpass</th>\n",
       "        <td>1.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Lowpass</th>\n",
       "        <td>45.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Filenames</th>\n",
       "        <td>Vipin_Apple.edf</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Duration</th>\n",
       "        <td>00:00:13 (HH:MM:SS)</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<RawEDF | Vipin_Apple.edf, 39 x 3250 (13.0 s), ~1.0 MB, data loaded>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.filter(1, 45, fir_design='firwin', picks=['eeg'])  # band-pass filter from 1 to 30 frequency over just\n",
    "                                                       # EEG channel and not EEG channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Measurement date</th>\n",
       "        \n",
       "        <td>February 01, 2017  12:10:40 GMT</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Experimenter</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "        <th>Participant</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Digitized points</th>\n",
       "        \n",
       "        <td>Not available</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>39 EEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>EOG channels</th>\n",
       "        <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ECG channels</th>\n",
       "        <td>Not available</td>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Sampling frequency</th>\n",
       "        <td>250.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Highpass</th>\n",
       "        <td>1.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Lowpass</th>\n",
       "        <td>45.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Projections</th>\n",
       "        <td>Average EEG reference : on</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Filenames</th>\n",
       "        <td>Vipin_Apple.edf</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Duration</th>\n",
       "        <td>00:00:13 (HH:MM:SS)</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<RawEDF | Vipin_Apple.edf, 39 x 3250 (13.0 s), ~1.0 MB, data loaded>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.set_eeg_reference('average', projection=True).apply_proj()  # re-referencing with the virtual average reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Info | 8 non-empty values\n",
      " bads: []\n",
      " ch_names: COUNTER, INTERPOLATED, AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, ...\n",
      " chs: 39 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 1.0 Hz\n",
      " lowpass: 45.0 Hz\n",
      " meas_date: 2017-02-01 12:10:40 UTC\n",
      " nchan: 39\n",
      " projs: Average EEG reference: on\n",
      " sfreq: 250.0 Hz\n",
      ">\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nSampling frequency (sfreq)\\nChannel names (ch_names)\\nChannel types (ch_types)\\nChannel units (units)\\nHigh-pass and low-pass filter settings (highpass, lowpass)\\nEEG reference (ref)\\nEEG electrode locations (chs[idx]['loc'])\\nMeasurement date and time (meas_date)\\nProjector information (projs)\\nBad channels (bads)\\nSensor positions (dig)\\nTrigger information (n_savesys and related fields)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw.info)\n",
    "'''\n",
    "Sampling frequency (sfreq)\n",
    "Channel names (ch_names)\n",
    "Channel types (ch_types)\n",
    "Channel units (units)\n",
    "High-pass and low-pass filter settings (highpass, lowpass)\n",
    "EEG reference (ref)\n",
    "EEG electrode locations (chs[idx]['loc'])\n",
    "Measurement date and time (meas_date)\n",
    "Projector information (projs)\n",
    "Bad channels (bads)\n",
    "Sensor positions (dig)\n",
    "Trigger information (n_savesys and related fields)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\Users\\sherl\\Downloads\\1Eldo-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -203.12 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "444 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "epochs = mne.read_epochs(\"C:\\\\Users\\\\sherl\\\\Downloads\\\\1Eldo-epo.fif\")\n",
    "# epochs1 = mne.read_epochs('../preprocessed/1Eldo-epo.fif')\n",
    "# epochs2 = mne.read_epochs('../preprocessed/2Eldo-epo.fif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = epochs1.get_data()\n",
    "# data2 = epochs2.get_data()\n",
    "# data = np.array(data1.tolist()+data2.tolist())\n",
    "data=epochs.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 14 channels (please be patient, this may take a while)\n",
      "Selecting by number: 14 components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sherl\\AppData\\Local\\Temp\\ipykernel_32576\\1906128710.py:2: RuntimeWarning: The epochs you passed to ICA.fit() were baseline-corrected. However, we suggest to fit ICA only on data that has been high-pass filtered, but NOT baseline-corrected.\n",
      "  ica.fit(epochs)                                      # Data decomposition with 50 components and fastica method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA took 2.1s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sherl\\AppData\\Local\\Temp\\ipykernel_32576\\1906128710.py:2: RuntimeWarning: Using n_components=14 (resulting in n_components_=14) may lead to an unstable mixing matrix estimation because the ratio between the largest (5.5) and smallest (4.3e-16) variances is too large (> 1e6); consider setting n_components=0.999999 or an integer <= 12\n",
      "  ica.fit(epochs)                                      # Data decomposition with 50 components and fastica method.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Method</th>\n",
       "        <td>fastica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit</th>\n",
       "        <td>21 iterations on epochs (125652 samples)</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>ICA components</th>\n",
       "        <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Available PCA components</th>\n",
       "        <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Channel types</th>\n",
       "        <td>eeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ICA components marked for exclusion</th>\n",
       "        <td>&mdash;</td>\n",
       "    </tr>\n",
       "    \n",
       "</table>"
      ],
      "text/plain": [
       "<ICA | epochs decomposition, method: fastica (fit in 21 iterations on 125652 samples), 14 ICA components (14 PCA components available), channel types: eeg, no sources marked for exclusion>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ica = mne.preprocessing.ICA(n_components=14, random_state=97, method='fastica')\n",
    "ica.fit(epochs)                                      # Data decomposition with 50 components and fastica method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying ICA to Epochs instance\n",
      "    Transforming to ICA space (14 components)\n",
      "    Zeroing out 3 ICA components\n",
      "    Projecting back using 14 PCA components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sherl\\AppData\\Local\\Temp\\ipykernel_32576\\2848464973.py:5: RuntimeWarning: The data you passed to ICA.apply() was baseline-corrected. Please note that ICA can introduce DC shifts, therefore you may wish to consider baseline-correcting the cleaned data again.\n",
      "  ica.apply(epochs)                       # Channels can be reconstructed using the ICA object’s apply()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Number of events</th>\n",
       "        <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Events</th>\n",
       "        \n",
       "        <td>label_IS1-COLD: 15<br/>label_IS1-DOCTOR: 15<br/>label_IS1-EATING: 15<br/>label_IS1-LIGHT: 15<br/>label_IS1-PAIN: 15<br/>label_IS1-SICK: 15<br/>label_IS1-TOILET: 15<br/>label_IS1-TV: 15<br/>label_IS1-WATER: 13<br/>label_IS1-YES: 14<br/>label_IS2-COLD: 14<br/>label_IS2-DOCTOR: 15<br/>label_IS2-EATING: 15<br/>label_IS2-LIGHT: 15<br/>label_IS2-PAIN: 15<br/>label_IS2-SICK: 15<br/>label_IS2-TOILET: 14<br/>label_IS2-TV: 15<br/>label_IS2-WATER: 15<br/>label_IS2-YES: 15<br/>label_IS3-COLD: 14<br/>label_IS3-DOCTOR: 15<br/>label_IS3-EATING: 15<br/>label_IS3-LIGHT: 15<br/>label_IS3-PAIN: 15<br/>label_IS3-SICK: 15<br/>label_IS3-TOILET: 15<br/>label_IS3-TV: 15<br/>label_IS3-WATER: 15<br/>label_IS3-YES: 15</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Time range</th>\n",
       "        <td>-0.203 – 2.000 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Baseline</th>\n",
       "        <td>-0.203 – 0.000 s</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<EpochsFIF |  444 events (all good), -0.203125 – 2 s, baseline -0.203125 – 0 s, ~13.4 MB, data loaded,\n",
       " 'label_IS1-COLD': 15\n",
       " 'label_IS1-DOCTOR': 15\n",
       " 'label_IS1-EATING': 15\n",
       " 'label_IS1-LIGHT': 15\n",
       " 'label_IS1-PAIN': 15\n",
       " 'label_IS1-SICK': 15\n",
       " 'label_IS1-TOILET': 15\n",
       " 'label_IS1-TV': 15\n",
       " 'label_IS1-WATER': 13\n",
       " 'label_IS1-YES': 14\n",
       " and 20 more events ...>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ica.exclude = [11, 26, 29, 30, 33, 34, 35, 36, 38, 44, 48, 49, 0, 6, 17] \n",
    "                                        # Put all comonent which you want to remove containg inspected (manual) \n",
    "                                        # [11, 26, 29, 30, 33, 34, 35, 36, 38, 44, 48, 49], EOG [0] and ECG [6,17] components\n",
    "                                        # Selected components are not real\n",
    "ica.apply(epochs)                       # Channels can be reconstructed using the ICA object’s apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sherl\\AppData\\Local\\Temp\\ipykernel_32576\\3336386786.py:1: RuntimeWarning: This filename (C:\\Users\\sherl\\Downloads\\sub-006_prerprocessed.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs.save(\"C:\\\\Users\\\\sherl\\\\Downloads\\\\sub-006_prerprocessed.fif\",overwrite=True)\n"
     ]
    }
   ],
   "source": [
    "epochs.save(\"C:\\\\Users\\\\sherl\\\\Downloads\\\\sub-006_prerprocessed.fif\",overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import os\n",
    "import os.path as op\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from mne_icalabel import label_components\n",
    "# from mayavi import mlab\n",
    "# %gui qt\n",
    "# %matplotlib qt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold,GridSearchCV,cross_val_score,cross_validate \n",
    "# Load necessary libraries\n",
    "import mne\n",
    "from mne.decoding import Vectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Models\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagined_words_dict ={ \"SICK\":0,\n",
    "\"COLD\":1,\n",
    "\"PAIN\":2,\n",
    "\"TOILET\":3,\n",
    "\"EATING\":4,\n",
    "\"WATER\":5,\n",
    "\"LIGHT\":6,\n",
    "\"DOCTOR\":7,\n",
    "\"YES\":8,\n",
    "\"TV\":9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_label(epochss):\n",
    "    labels = epochss.events[:, -1].copy()\n",
    "    event_id = epochss.event_id\n",
    "    for i in range(len(labels)):\n",
    "        label_key =list(event_id.keys())[list(event_id.values()).index(labels[i])]\n",
    "        start_index =label_key.find(\"-\")\n",
    "        label_key=label_key[start_index+1:]\n",
    "        labels[i] =imagined_words_dict[label_key]\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels1 = set_label(epochs1)\n",
    "# labels2 = set_label(epochs2)\n",
    "# labels = np.array(labels1.tolist() + labels2.tolist())\n",
    "labels = set_label(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "def read_split_epoch(split):\n",
    "    epochs = mne.read_epochs(\"C:\\\\Users\\\\sherl\\\\Downloads\\\\1Eldo-epo.fif\")\n",
    "    data = epochs.get_data()\n",
    "    labels = set_label(epochs)\n",
    "    # Split the data into training and testing sets\n",
    "    train_epoch, test_epoch, tr_labels, ts_labels = train_test_split(data, labels,shuffle=True)\n",
    "    return train_epoch, test_epoch, tr_labels, ts_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\Users\\sherl\\Downloads\\1Eldo-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -203.12 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "444 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "tr_ep_data,ts_ep_data,tr_labels,ts_labels = read_split_epoch(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_window_data,tr_window_labels = make_windowing(tr_ep_data,tr_labels)\n",
    "ts_window_data,ts_window_labels = make_windowing(ts_ep_data,ts_labels)\n",
    "#window_data,window_labels =data,labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feature, test_feature, train_label, test_label = train_test_split(data_seg_feature, data_seg_label, shuffle=True)\n",
    "\n",
    "# train_epoch, test_epoch, tr_labels, ts_labels = train_test_split(data, labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    "# tr_ep_data,ts_ep_data,tr_labels,ts_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature, test_feature, train_label, test_label =tr_ep_data,ts_ep_data,tr_labels,ts_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_length=283 # segment_length= sampling_freuncy* stride_length | 25=250*0.1\n",
    "no_feature=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalization\n",
    "# # before normalize reshape data back to raw data shape\n",
    "# train_feature_2d = train_feature.reshape([-1, no_feature])\n",
    "# test_feature_2d = test_feature.reshape([-1, no_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Assuming your original data has shape (samples, features, time_steps)\n",
    "# reshaped_train_feature = train_feature.reshape(-1, train_feature.shape[-1])\n",
    "# reshaped_test_feature = test_feature.reshape(-1, test_feature.shape[-1])\n",
    "\n",
    "# scaler3 = MinMaxScaler().fit(reshaped_train_feature)\n",
    "\n",
    "# train_fea_norm1 = scaler3.transform(reshaped_train_feature)\n",
    "# test_fea_norm1 = scaler3.transform(reshaped_test_feature)\n",
    "\n",
    "# print('After normalization, the shape of training feature:', train_fea_norm1.shape,\n",
    "#       '\\nAfter normalization, the shape of test feature:', test_fea_norm1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # after normalization, reshape data to 3d in order to feed in to LSTM\n",
    "# train_fea_norm1 = train_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "# test_fea_norm1 = test_fea_norm1.reshape([-1, segment_length, no_feature])\n",
    "# print('After reshape, the shape of training feature:', train_fea_norm1.shape,\n",
    "#       '\\nAfter reshape, the shape of test feature:', test_fea_norm1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fea_norm1 , test_fea_norm1 = train_feature, test_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_size = train_fea_norm1.shape[0] # use test_data as batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cpu now.\n"
     ]
    }
   ],
   "source": [
    "# check if a GPU is available\n",
    "with_gpu = torch.cuda.is_available()\n",
    "if with_gpu:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('We are using %s now.' %device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data segmentation\n",
    "n_class = 10  # 0~9 classes\n",
    "no_feature = 14  # the number of the features\n",
    "segment_length = 283  # selected time window;\n",
    "LR = 0.001  # learning rate\n",
    "EPOCH = 8880 #after windowing\n",
    "n_hidden = 64  # number of neurons in hidden layer\n",
    "l2 = 0.005  # the coefficient of l2-norm regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU(\n",
      "  (gru_layer): GRU(14, 64, num_layers=2, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# classifier\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.gru_layer = nn.GRU(\n",
    "            input_size=no_feature,\n",
    "            hidden_size=n_hidden,\n",
    "            num_layers=2,\n",
    "            bias=True,\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, segment_length, no_feature)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r_out, (h_n, h_c) = self.gru_layer(x.float(), None)\n",
    "        r_out = F.dropout(r_out, 0.3)\n",
    "        test_output = self.out(r_out[:, -1, :]) # choose r_out at the last time step\n",
    "        return test_output\n",
    "\n",
    "gru = GRU()\n",
    "gru.to(device)\n",
    "print(gru)\n",
    "\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=LR, weight_decay=l2)   # optimize all parameters\n",
    "#loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed data into dataloader\n",
    "train_fea_norm1 = torch.tensor(train_fea_norm1).to(device)\n",
    "train_label = torch.tensor(train_label.flatten()).to(device)\n",
    "train_data = Data.TensorDataset(train_fea_norm1, train_label)\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_size, shuffle=False)\n",
    "\n",
    "test_fea_norm1 = torch.tensor(test_fea_norm1).to(device)\n",
    "test_label = torch.tensor(test_label.flatten()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_fea_norm1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_fea_norm1=test_fea_norm1.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  train ACC: 0.0871 test ACC: 0.1081 | AUC: 0.5194\n",
      "Epoch:  10  train ACC: 0.0961 test ACC: 0.0901 | AUC: 0.5313\n",
      "Epoch:  20  train ACC: 0.0931 test ACC: 0.0180 | AUC: 0.4377\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m step, (train_x, train_y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m      9\u001b[0m     train_x\u001b[39m=\u001b[39mtrain_x\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     output \u001b[39m=\u001b[39m gru(train_x)  \u001b[39m# GRU output of training data\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[39m#loss = loss_func(output, train_y.long())  # cross entropy loss\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# clear gradients for this training step\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[31], line 17\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 17\u001b[0m     r_out, (h_n, h_c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgru_layer(x\u001b[39m.\u001b[39;49mfloat(), \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     18\u001b[0m     r_out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(r_out, \u001b[39m0.3\u001b[39m)\n\u001b[0;32m     19\u001b[0m     test_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(r_out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]) \u001b[39m# choose r_out at the last time step\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\rnn.py:998\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    997\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 998\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    999\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m   1000\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1001\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m   1002\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = []\n",
    "best_auc = []\n",
    "\n",
    "# training and testing\n",
    "start_time = time.perf_counter()\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (train_x, train_y) in enumerate(train_loader):\n",
    "\n",
    "        train_x=train_x.transpose(1,2)\n",
    "        output = gru(train_x)  # GRU output of training data\n",
    "        #loss = loss_func(output, train_y.long())  # cross entropy loss\n",
    "        optimizer.zero_grad()  # clear gradients for this training step\n",
    "        #loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients\n",
    "\n",
    "    test_fea_norm1=test_fea_norm1.transpose(1,2)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        #test_fea_norm1=test_fea_norm1.transpose(1,2)\n",
    "        test_output = gru(test_fea_norm1)  # GRU output of test data\n",
    "        #test_loss = loss_func(test_output, test_label.long())\n",
    "\n",
    "        test_y_score = one_hot(test_label.data.cpu().numpy())\n",
    "        pred_score = F.softmax(test_output, dim=1).data.cpu().numpy()  # normalize the output\n",
    "        auc_score = roc_auc_score(test_y_score, pred_score)\n",
    "\n",
    "        pred_y = torch.max(test_output, 1)[1].data.cpu().numpy()\n",
    "        pred_train = torch.max(output, 1)[1].data.cpu().numpy()\n",
    "\n",
    "        test_acc = accuracy_score(test_label.data.cpu().numpy(), pred_y)\n",
    "        train_acc = accuracy_score(train_y.data.cpu().numpy(), pred_train)\n",
    "\n",
    "        print('Epoch: ', epoch, \n",
    "              #'|train loss: %.4f' % loss.data.item(),\n",
    "              ' train ACC: %.4f' % train_acc, \n",
    "              #'| test loss: %.4f' % test_loss.item(),\n",
    "              'test ACC: %.4f' % test_acc, '| AUC: %.4f' % auc_score)\n",
    "        best_acc.append(test_acc)\n",
    "        best_auc.append(auc_score)\n",
    "current_time = time.perf_counter()\n",
    "running_time = current_time - start_time\n",
    "\n",
    "print(classification_report(test_label.data.numpy(), pred_y))\n",
    "print('BEST TEST ACC: {}, AUC: {}'.format(max(best_acc), max(best_auc)))\n",
    "print(\"Total Running Time: {} seconds\".format(round(running_time, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

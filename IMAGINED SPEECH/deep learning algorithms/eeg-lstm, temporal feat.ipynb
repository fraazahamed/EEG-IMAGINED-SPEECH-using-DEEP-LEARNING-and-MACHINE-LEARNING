{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,classification_report\n",
    "\n",
    "import sys\n",
    "\n",
    "# Add the directory containing the module to the Python path\n",
    "sys.path.append(\"C:\\\\Users\\\\sherl\\\\Downloads\\\\nit pendrive\\\\Deep-Learning-for-BCI-master\\\\Deep-Learning-for-BCI-master\\\\tutorial\")\n",
    "\n",
    "# Now you can import the module\n",
    "import myimporter\n",
    "\n",
    "\n",
    "import myimporter\n",
    "from BCI_functions import *  # BCI_functions.ipynb contains some functions we might use multiple times in this tutorial\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import os.path as op\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windowing(X,Y):\n",
    "    X_new = []\n",
    "    Y_new = []\n",
    "    npt = 128 # .5 sec\n",
    "    stride = 8 # around .1 sec\n",
    "    ctr = 0\n",
    "    for i in range(0,X.shape[0]):\n",
    "        y = Y[i]\n",
    "        a= X[i,:,:]\n",
    "        a = a.transpose()\n",
    "        a.shape\n",
    "        val = 0\n",
    "        kd=len(a)\n",
    "        while val<=(len(a)-npt):\n",
    "            x = a[val:val+npt,:]\n",
    "\n",
    "            X_new.append(x.T)\n",
    "            Y_new.append(y)\n",
    "            val = val+stride\n",
    "            \n",
    "    return np.array(X_new),np.array(Y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\sherl\\Downloads\\Vipin_Apple.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    }
   ],
   "source": [
    "# Load .edf file\n",
    "filename = \"C:\\\\Users\\\\sherl\\\\Downloads\\\\Vipin_Apple.edf\"\n",
    "raw = mne.io.read_raw_edf(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Measurement date</th>\n",
       "        \n",
       "        <td>February 01, 2017  12:10:40 GMT</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Experimenter</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "        <th>Participant</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Digitized points</th>\n",
       "        \n",
       "        <td>Not available</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>39 EEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>EOG channels</th>\n",
       "        <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ECG channels</th>\n",
       "        <td>Not available</td>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Sampling frequency</th>\n",
       "        <td>250.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Highpass</th>\n",
       "        <td>0.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Lowpass</th>\n",
       "        <td>64.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Filenames</th>\n",
       "        <td>Vipin_Apple.edf</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Duration</th>\n",
       "        <td>00:00:13 (HH:MM:SS)</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<RawEDF | Vipin_Apple.edf, 39 x 3250 (13.0 s), ~1.0 MB, data loaded>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.resample(250, npad=\"auto\")    # set sampling frequency to 256 points per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-6 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Measurement date</th>\n",
       "        \n",
       "        <td>February 01, 2017  12:10:40 GMT</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Experimenter</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "        <th>Participant</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Digitized points</th>\n",
       "        \n",
       "        <td>Not available</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>39 EEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>EOG channels</th>\n",
       "        <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ECG channels</th>\n",
       "        <td>Not available</td>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Sampling frequency</th>\n",
       "        <td>250.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Highpass</th>\n",
       "        <td>1.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Lowpass</th>\n",
       "        <td>45.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Filenames</th>\n",
       "        <td>Vipin_Apple.edf</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Duration</th>\n",
       "        <td>00:00:13 (HH:MM:SS)</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<RawEDF | Vipin_Apple.edf, 39 x 3250 (13.0 s), ~1.0 MB, data loaded>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.filter(1, 45, fir_design='firwin', picks=['eeg'])  # band-pass filter from 1 to 30 frequency over just\n",
    "                                                       # EEG channel and not EEG channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Measurement date</th>\n",
       "        \n",
       "        <td>February 01, 2017  12:10:40 GMT</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Experimenter</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "        <th>Participant</th>\n",
       "        \n",
       "        <td>Unknown</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Digitized points</th>\n",
       "        \n",
       "        <td>Not available</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>39 EEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>EOG channels</th>\n",
       "        <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ECG channels</th>\n",
       "        <td>Not available</td>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Sampling frequency</th>\n",
       "        <td>250.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Highpass</th>\n",
       "        <td>1.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Lowpass</th>\n",
       "        <td>45.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Projections</th>\n",
       "        <td>Average EEG reference : on</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Filenames</th>\n",
       "        <td>Vipin_Apple.edf</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Duration</th>\n",
       "        <td>00:00:13 (HH:MM:SS)</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<RawEDF | Vipin_Apple.edf, 39 x 3250 (13.0 s), ~1.0 MB, data loaded>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.set_eeg_reference('average', projection=True).apply_proj()  # re-referencing with the virtual average reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Info | 8 non-empty values\n",
      " bads: []\n",
      " ch_names: COUNTER, INTERPOLATED, AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, ...\n",
      " chs: 39 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 1.0 Hz\n",
      " lowpass: 45.0 Hz\n",
      " meas_date: 2017-02-01 12:10:40 UTC\n",
      " nchan: 39\n",
      " projs: Average EEG reference: on\n",
      " sfreq: 250.0 Hz\n",
      ">\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nSampling frequency (sfreq)\\nChannel names (ch_names)\\nChannel types (ch_types)\\nChannel units (units)\\nHigh-pass and low-pass filter settings (highpass, lowpass)\\nEEG reference (ref)\\nEEG electrode locations (chs[idx]['loc'])\\nMeasurement date and time (meas_date)\\nProjector information (projs)\\nBad channels (bads)\\nSensor positions (dig)\\nTrigger information (n_savesys and related fields)\\n\""
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw.info)\n",
    "'''\n",
    "Sampling frequency (sfreq)\n",
    "Channel names (ch_names)\n",
    "Channel types (ch_types)\n",
    "Channel units (units)\n",
    "High-pass and low-pass filter settings (highpass, lowpass)\n",
    "EEG reference (ref)\n",
    "EEG electrode locations (chs[idx]['loc'])\n",
    "Measurement date and time (meas_date)\n",
    "Projector information (projs)\n",
    "Bad channels (bads)\n",
    "Sensor positions (dig)\n",
    "Trigger information (n_savesys and related fields)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\Users\\sherl\\Downloads\\1Eldo-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -203.12 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "444 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "epochs = mne.read_epochs(\"C:\\\\Users\\\\sherl\\\\Downloads\\\\1Eldo-epo.fif\")\n",
    "# epochs1 = mne.read_epochs('../preprocessed/1Eldo-epo.fif')\n",
    "# epochs2 = mne.read_epochs('../preprocessed/2Eldo-epo.fif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = epochs1.get_data()\n",
    "# data2 = epochs2.get_data()\n",
    "# data = np.array(data1.tolist()+data2.tolist())\n",
    "data=epochs.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA to data using 14 channels (please be patient, this may take a while)\n",
      "Selecting by number: 14 components\n",
      "Fitting ICA took 1.0s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Method</th>\n",
       "        <td>fastica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Fit</th>\n",
       "        <td>21 iterations on epochs (125652 samples)</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>ICA components</th>\n",
       "        <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Available PCA components</th>\n",
       "        <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Channel types</th>\n",
       "        <td>eeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ICA components marked for exclusion</th>\n",
       "        <td>&mdash;</td>\n",
       "    </tr>\n",
       "    \n",
       "</table>"
      ],
      "text/plain": [
       "<ICA | epochs decomposition, method: fastica (fit in 21 iterations on 125652 samples), 14 ICA components (14 PCA components available), channel types: eeg, no sources marked for exclusion>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ica = mne.preprocessing.ICA(n_components=14, random_state=97, method='fastica')\n",
    "ica.fit(epochs)                                      # Data decomposition with 50 components and fastica method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying ICA to Epochs instance\n",
      "    Transforming to ICA space (14 components)\n",
      "    Zeroing out 3 ICA components\n",
      "    Projecting back using 14 PCA components\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Number of events</th>\n",
       "        <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Events</th>\n",
       "        \n",
       "        <td>label_IS1-COLD: 15<br/>label_IS1-DOCTOR: 15<br/>label_IS1-EATING: 15<br/>label_IS1-LIGHT: 15<br/>label_IS1-PAIN: 15<br/>label_IS1-SICK: 15<br/>label_IS1-TOILET: 15<br/>label_IS1-TV: 15<br/>label_IS1-WATER: 13<br/>label_IS1-YES: 14<br/>label_IS2-COLD: 14<br/>label_IS2-DOCTOR: 15<br/>label_IS2-EATING: 15<br/>label_IS2-LIGHT: 15<br/>label_IS2-PAIN: 15<br/>label_IS2-SICK: 15<br/>label_IS2-TOILET: 14<br/>label_IS2-TV: 15<br/>label_IS2-WATER: 15<br/>label_IS2-YES: 15<br/>label_IS3-COLD: 14<br/>label_IS3-DOCTOR: 15<br/>label_IS3-EATING: 15<br/>label_IS3-LIGHT: 15<br/>label_IS3-PAIN: 15<br/>label_IS3-SICK: 15<br/>label_IS3-TOILET: 15<br/>label_IS3-TV: 15<br/>label_IS3-WATER: 15<br/>label_IS3-YES: 15</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Time range</th>\n",
       "        <td>-0.203 – 2.000 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Baseline</th>\n",
       "        <td>-0.203 – 0.000 s</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<EpochsFIF |  444 events (all good), -0.203125 – 2 s, baseline -0.203125 – 0 s, ~13.4 MB, data loaded,\n",
       " 'label_IS1-COLD': 15\n",
       " 'label_IS1-DOCTOR': 15\n",
       " 'label_IS1-EATING': 15\n",
       " 'label_IS1-LIGHT': 15\n",
       " 'label_IS1-PAIN': 15\n",
       " 'label_IS1-SICK': 15\n",
       " 'label_IS1-TOILET': 15\n",
       " 'label_IS1-TV': 15\n",
       " 'label_IS1-WATER': 13\n",
       " 'label_IS1-YES': 14\n",
       " and 20 more events ...>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ica.exclude = [11, 26, 29, 30, 33, 34, 35, 36, 38, 44, 48, 49, 0, 6, 17] \n",
    "                                        # Put all comonent which you want to remove containg inspected (manual) \n",
    "                                        # [11, 26, 29, 30, 33, 34, 35, 36, 38, 44, 48, 49], EOG [0] and ECG [6,17] components\n",
    "                                        # Selected components are not real\n",
    "ica.apply(epochs)                       # Channels can be reconstructed using the ICA object’s apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file.\n"
     ]
    }
   ],
   "source": [
    "epochs.save(\"C:\\\\Users\\\\sherl\\\\Downloads\\\\sub-006_prerprocessed.fif\",overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import os\n",
    "import os.path as op\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from mne_icalabel import label_components\n",
    "# from mayavi import mlab\n",
    "# %gui qt\n",
    "# %matplotlib qt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold,GridSearchCV,cross_val_score,cross_validate \n",
    "# Load necessary libraries\n",
    "import mne\n",
    "from mne.decoding import Vectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Models\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagined_words_dict ={ \"SICK\":0,\n",
    "\"COLD\":1,\n",
    "\"PAIN\":2,\n",
    "\"TOILET\":3,\n",
    "\"EATING\":4,\n",
    "\"WATER\":5,\n",
    "\"LIGHT\":6,\n",
    "\"DOCTOR\":7,\n",
    "\"YES\":8,\n",
    "\"TV\":9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_label(epochss):\n",
    "    labels = epochss.events[:, -1].copy()\n",
    "    event_id = epochss.event_id\n",
    "    for i in range(len(labels)):\n",
    "        label_key =list(event_id.keys())[list(event_id.values()).index(labels[i])]\n",
    "        start_index =label_key.find(\"-\")\n",
    "        label_key=label_key[start_index+1:]\n",
    "        labels[i] =imagined_words_dict[label_key]\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels1 = set_label(epochs1)\n",
    "# labels2 = set_label(epochs2)\n",
    "# labels = np.array(labels1.tolist() + labels2.tolist())\n",
    "labels = set_label(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "def read_split_epoch(split):\n",
    "    epochs = mne.read_epochs(\"C:\\\\Users\\\\sherl\\\\Downloads\\\\1Eldo-epo.fif\")\n",
    "    data = epochs.get_data()\n",
    "    labels = set_label(epochs)\n",
    "    # Split the data into training and testing sets\n",
    "    train_epoch, test_epoch, tr_labels, ts_labels = train_test_split(data, labels,shuffle=True)\n",
    "    return train_epoch, test_epoch, tr_labels, ts_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(no_feature*segment_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\Users\\sherl\\Downloads\\1Eldo-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =    -203.12 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "444 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "tr_ep_data,ts_ep_data,tr_labels,ts_labels = read_split_epoch(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_window_data,tr_window_labels = make_windowing(tr_ep_data,tr_labels)\n",
    "ts_window_data,ts_window_labels = make_windowing(ts_ep_data,ts_labels)\n",
    "#window_data,window_labels =data,labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feature, test_feature, train_label, test_label = train_test_split(data_seg_feature, data_seg_label, shuffle=True)\n",
    "\n",
    "# train_epoch, test_epoch, tr_labels, ts_labels = train_test_split(data, labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    "# tr_ep_data,ts_ep_data,tr_labels,ts_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature, test_feature, train_label, test_label =tr_ep_data,ts_ep_data,tr_labels,ts_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_length=283 # segment_length= sampling_freuncy* stride_length | 25=250*0.1\n",
    "no_feature=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using cpu now.\n"
     ]
    }
   ],
   "source": [
    "# check if a GPU is available\n",
    "with_gpu = torch.cuda.is_available()\n",
    "if with_gpu:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('We are using %s now.' %device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fea_norm1 , test_fea_norm1 = train_feature, test_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_size = test_fea_norm1.shape[0] # use test_data as batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed data into dataloader\n",
    "train_fea_norm1 = torch.tensor(train_fea_norm1).to(device)\n",
    "train_label = torch.tensor(train_label.flatten()).to(device)\n",
    "train_data = Data.TensorDataset(train_fea_norm1, train_label)\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_size, shuffle=False)\n",
    "\n",
    "test_fea_norm1 = torch.tensor(test_fea_norm1).to(device)\n",
    "test_label = torch.tensor(test_label.flatten()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = 10\n",
    "no_feature = 14  # the number of the features\n",
    "segment_length = 283  # selected time window; \n",
    "LR = 0.005  # learning rate\n",
    "EPOCH = 8880 #after windowing\n",
    "n_hidden = 128  # number of neurons in hidden layer\n",
    "l2 = 0.001  # the coefficient of l2-norm regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm_layer): LSTM(14, 128, num_layers=2, batch_first=True)\n",
      "  (out): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# classifier\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size=no_feature,\n",
    "            hidden_size=n_hidden,         # LSTM hidden unit\n",
    "            num_layers=2,           # number of LSTM layer\n",
    "            bias=True,\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, segment_length, no_feature)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r_out, (h_n, h_c) = self.lstm_layer(x.float(), None)\n",
    "        r_out = F.dropout(r_out, 0.3)\n",
    "\n",
    "        test_output = self.out(r_out[:, -1, :]) # choose r_out at the last time step\n",
    "        return test_output\n",
    "\n",
    "lstm = LSTM()\n",
    "lstm.to(device)\n",
    "print(lstm)\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=LR, weight_decay=l2)   # optimize all parameters\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 |train loss: 2.3081  train ACC: 0.0721 | test loss: 2.3136 test ACC: 0.0541 | AUC: 0.4969\n",
      "Epoch:  10 |train loss: 2.3071  train ACC: 0.1081 | test loss: 2.3160 test ACC: 0.0811 | AUC: 0.4637\n",
      "Epoch:  20 |train loss: 2.3080  train ACC: 0.0991 | test loss: 2.3129 test ACC: 0.0901 | AUC: 0.5444\n",
      "Epoch:  30 |train loss: 2.3069  train ACC: 0.1261 | test loss: 2.3125 test ACC: 0.0541 | AUC: 0.5200\n",
      "Epoch:  40 |train loss: 2.3080  train ACC: 0.0811 | test loss: 2.3150 test ACC: 0.0721 | AUC: 0.4858\n",
      "Epoch:  50 |train loss: 2.3051  train ACC: 0.0991 | test loss: 2.3130 test ACC: 0.0901 | AUC: 0.5081\n",
      "Epoch:  60 |train loss: 2.3061  train ACC: 0.1351 | test loss: 2.3130 test ACC: 0.0631 | AUC: 0.5011\n",
      "Epoch:  70 |train loss: 2.3038  train ACC: 0.0991 | test loss: 2.3132 test ACC: 0.0721 | AUC: 0.5237\n",
      "Epoch:  80 |train loss: 2.3062  train ACC: 0.0991 | test loss: 2.3127 test ACC: 0.0901 | AUC: 0.4952\n",
      "Epoch:  90 |train loss: 2.3075  train ACC: 0.0901 | test loss: 2.3125 test ACC: 0.1171 | AUC: 0.5396\n",
      "Epoch:  100 |train loss: 2.3064  train ACC: 0.0811 | test loss: 2.3126 test ACC: 0.0901 | AUC: 0.5555\n",
      "Epoch:  110 |train loss: 2.3084  train ACC: 0.0901 | test loss: 2.3144 test ACC: 0.0541 | AUC: 0.4603\n",
      "Epoch:  120 |train loss: 2.3070  train ACC: 0.0991 | test loss: 2.3132 test ACC: 0.0721 | AUC: 0.4990\n",
      "Epoch:  130 |train loss: 2.3068  train ACC: 0.0901 | test loss: 2.3130 test ACC: 0.1081 | AUC: 0.5269\n",
      "Epoch:  140 |train loss: 2.3067  train ACC: 0.0901 | test loss: 2.3136 test ACC: 0.0901 | AUC: 0.5112\n",
      "Epoch:  150 |train loss: 2.3064  train ACC: 0.0541 | test loss: 2.3128 test ACC: 0.0901 | AUC: 0.4727\n",
      "Epoch:  160 |train loss: 2.3067  train ACC: 0.0991 | test loss: 2.3132 test ACC: 0.0631 | AUC: 0.4955\n",
      "Epoch:  170 |train loss: 2.3069  train ACC: 0.0631 | test loss: 2.3133 test ACC: 0.1171 | AUC: 0.5477\n",
      "Epoch:  180 |train loss: 2.3071  train ACC: 0.0721 | test loss: 2.3137 test ACC: 0.0541 | AUC: 0.4565\n",
      "Epoch:  190 |train loss: 2.3067  train ACC: 0.1261 | test loss: 2.3141 test ACC: 0.0450 | AUC: 0.4689\n",
      "Epoch:  200 |train loss: 2.3070  train ACC: 0.0901 | test loss: 2.3140 test ACC: 0.0631 | AUC: 0.5012\n",
      "Epoch:  210 |train loss: 2.3067  train ACC: 0.0360 | test loss: 2.3136 test ACC: 0.0811 | AUC: 0.5093\n",
      "Epoch:  220 |train loss: 2.3063  train ACC: 0.0991 | test loss: 2.3141 test ACC: 0.0631 | AUC: 0.5088\n",
      "Epoch:  230 |train loss: 2.3069  train ACC: 0.0991 | test loss: 2.3135 test ACC: 0.0811 | AUC: 0.5131\n",
      "Epoch:  240 |train loss: 2.3073  train ACC: 0.0811 | test loss: 2.3139 test ACC: 0.0721 | AUC: 0.4813\n",
      "Epoch:  250 |train loss: 2.3060  train ACC: 0.0991 | test loss: 2.3147 test ACC: 0.0721 | AUC: 0.4700\n",
      "Epoch:  260 |train loss: 2.3069  train ACC: 0.1081 | test loss: 2.3123 test ACC: 0.1081 | AUC: 0.5548\n",
      "Epoch:  270 |train loss: 2.3060  train ACC: 0.0631 | test loss: 2.3147 test ACC: 0.0450 | AUC: 0.4251\n",
      "Epoch:  280 |train loss: 2.3076  train ACC: 0.0811 | test loss: 2.3140 test ACC: 0.0721 | AUC: 0.5087\n",
      "Epoch:  290 |train loss: 2.3064  train ACC: 0.0631 | test loss: 2.3122 test ACC: 0.0811 | AUC: 0.5447\n",
      "Epoch:  300 |train loss: 2.3061  train ACC: 0.1081 | test loss: 2.3148 test ACC: 0.0811 | AUC: 0.4694\n",
      "Epoch:  310 |train loss: 2.3065  train ACC: 0.1081 | test loss: 2.3140 test ACC: 0.0901 | AUC: 0.5228\n",
      "Epoch:  320 |train loss: 2.3063  train ACC: 0.0991 | test loss: 2.3143 test ACC: 0.0811 | AUC: 0.5140\n",
      "Epoch:  330 |train loss: 2.3052  train ACC: 0.0901 | test loss: 2.3126 test ACC: 0.0811 | AUC: 0.5277\n",
      "Epoch:  340 |train loss: 2.3068  train ACC: 0.0901 | test loss: 2.3138 test ACC: 0.0631 | AUC: 0.4941\n",
      "Epoch:  350 |train loss: 2.3069  train ACC: 0.0901 | test loss: 2.3122 test ACC: 0.0901 | AUC: 0.5273\n",
      "Epoch:  360 |train loss: 2.3075  train ACC: 0.0901 | test loss: 2.3137 test ACC: 0.0631 | AUC: 0.5156\n",
      "Epoch:  370 |train loss: 2.3066  train ACC: 0.0721 | test loss: 2.3134 test ACC: 0.0811 | AUC: 0.5301\n",
      "Epoch:  380 |train loss: 2.3062  train ACC: 0.0991 | test loss: 2.3142 test ACC: 0.0631 | AUC: 0.5096\n",
      "Epoch:  390 |train loss: 2.3077  train ACC: 0.0991 | test loss: 2.3144 test ACC: 0.0811 | AUC: 0.4959\n",
      "Epoch:  400 |train loss: 2.3055  train ACC: 0.0991 | test loss: 2.3160 test ACC: 0.0450 | AUC: 0.4035\n",
      "Epoch:  410 |train loss: 2.3082  train ACC: 0.0721 | test loss: 2.3140 test ACC: 0.0901 | AUC: 0.5310\n",
      "Epoch:  420 |train loss: 2.3067  train ACC: 0.0991 | test loss: 2.3133 test ACC: 0.0721 | AUC: 0.5152\n",
      "Epoch:  430 |train loss: 2.3061  train ACC: 0.0811 | test loss: 2.3135 test ACC: 0.0631 | AUC: 0.4659\n",
      "Epoch:  440 |train loss: 2.3069  train ACC: 0.0991 | test loss: 2.3143 test ACC: 0.0631 | AUC: 0.4349\n",
      "Epoch:  450 |train loss: 2.3076  train ACC: 0.0631 | test loss: 2.3142 test ACC: 0.0631 | AUC: 0.4822\n",
      "Epoch:  460 |train loss: 2.3095  train ACC: 0.1261 | test loss: 2.3144 test ACC: 0.0811 | AUC: 0.5181\n",
      "Epoch:  470 |train loss: 2.3057  train ACC: 0.0901 | test loss: 2.3133 test ACC: 0.0811 | AUC: 0.5423\n",
      "Epoch:  480 |train loss: 2.3053  train ACC: 0.0991 | test loss: 2.3137 test ACC: 0.0811 | AUC: 0.4610\n",
      "Epoch:  490 |train loss: 2.3067  train ACC: 0.0901 | test loss: 2.3134 test ACC: 0.0721 | AUC: 0.5030\n",
      "Epoch:  500 |train loss: 2.3069  train ACC: 0.0811 | test loss: 2.3136 test ACC: 0.0721 | AUC: 0.5349\n",
      "Epoch:  510 |train loss: 2.3072  train ACC: 0.0811 | test loss: 2.3134 test ACC: 0.1081 | AUC: 0.5425\n",
      "Epoch:  520 |train loss: 2.3065  train ACC: 0.1171 | test loss: 2.3146 test ACC: 0.0721 | AUC: 0.4459\n",
      "Epoch:  530 |train loss: 2.3070  train ACC: 0.1171 | test loss: 2.3134 test ACC: 0.0721 | AUC: 0.5105\n",
      "Epoch:  540 |train loss: 2.3098  train ACC: 0.1081 | test loss: 2.3142 test ACC: 0.0721 | AUC: 0.4726\n",
      "Epoch:  550 |train loss: 2.3090  train ACC: 0.1081 | test loss: 2.3089 test ACC: 0.1261 | AUC: 0.5582\n",
      "Epoch:  560 |train loss: 2.3088  train ACC: 0.0541 | test loss: 2.3133 test ACC: 0.0811 | AUC: 0.5252\n",
      "Epoch:  570 |train loss: 2.3075  train ACC: 0.0631 | test loss: 2.3169 test ACC: 0.0721 | AUC: 0.4595\n",
      "Epoch:  580 |train loss: 2.3075  train ACC: 0.0811 | test loss: 2.3132 test ACC: 0.1081 | AUC: 0.5099\n",
      "Epoch:  590 |train loss: 2.3070  train ACC: 0.0721 | test loss: 2.3127 test ACC: 0.1081 | AUC: 0.5306\n",
      "Epoch:  600 |train loss: 2.3061  train ACC: 0.1081 | test loss: 2.3148 test ACC: 0.0901 | AUC: 0.4977\n",
      "Epoch:  610 |train loss: 2.3066  train ACC: 0.0901 | test loss: 2.3141 test ACC: 0.0721 | AUC: 0.5009\n",
      "Epoch:  620 |train loss: 2.3056  train ACC: 0.0901 | test loss: 2.3143 test ACC: 0.0811 | AUC: 0.4734\n",
      "Epoch:  630 |train loss: 2.3061  train ACC: 0.0901 | test loss: 2.3150 test ACC: 0.0721 | AUC: 0.4480\n",
      "Epoch:  640 |train loss: 2.3059  train ACC: 0.1081 | test loss: 2.3134 test ACC: 0.0631 | AUC: 0.5064\n",
      "Epoch:  650 |train loss: 2.3063  train ACC: 0.0721 | test loss: 2.3153 test ACC: 0.0631 | AUC: 0.4211\n",
      "Epoch:  660 |train loss: 2.3067  train ACC: 0.1081 | test loss: 2.3139 test ACC: 0.0811 | AUC: 0.4941\n",
      "Epoch:  670 |train loss: 2.3055  train ACC: 0.1081 | test loss: 2.3142 test ACC: 0.0811 | AUC: 0.4982\n",
      "Epoch:  680 |train loss: 2.3090  train ACC: 0.0811 | test loss: 2.3141 test ACC: 0.0901 | AUC: 0.5177\n",
      "Epoch:  690 |train loss: 2.3065  train ACC: 0.0901 | test loss: 2.3146 test ACC: 0.0721 | AUC: 0.4846\n",
      "Epoch:  700 |train loss: 2.3056  train ACC: 0.0991 | test loss: 2.3136 test ACC: 0.0721 | AUC: 0.5144\n",
      "Epoch:  710 |train loss: 2.3058  train ACC: 0.0991 | test loss: 2.3150 test ACC: 0.0901 | AUC: 0.4408\n",
      "Epoch:  720 |train loss: 2.3067  train ACC: 0.0901 | test loss: 2.3141 test ACC: 0.0991 | AUC: 0.5033\n",
      "Epoch:  730 |train loss: 2.3061  train ACC: 0.0631 | test loss: 2.3127 test ACC: 0.0901 | AUC: 0.5229\n",
      "Epoch:  740 |train loss: 2.3072  train ACC: 0.0901 | test loss: 2.3145 test ACC: 0.0901 | AUC: 0.4640\n",
      "Epoch:  750 |train loss: 2.3067  train ACC: 0.0901 | test loss: 2.3136 test ACC: 0.1081 | AUC: 0.5218\n",
      "Epoch:  760 |train loss: 2.3084  train ACC: 0.0901 | test loss: 2.3142 test ACC: 0.0721 | AUC: 0.4946\n",
      "Epoch:  770 |train loss: 2.3050  train ACC: 0.1081 | test loss: 2.3146 test ACC: 0.0631 | AUC: 0.4604\n",
      "Epoch:  780 |train loss: 2.3066  train ACC: 0.0811 | test loss: 2.3143 test ACC: 0.0811 | AUC: 0.4976\n",
      "Epoch:  790 |train loss: 2.3079  train ACC: 0.0541 | test loss: 2.3149 test ACC: 0.1081 | AUC: 0.4769\n",
      "Epoch:  800 |train loss: 2.3110  train ACC: 0.0901 | test loss: 2.3170 test ACC: 0.0811 | AUC: 0.5027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m loss \u001b[39m=\u001b[39m loss_func(output, train_y\u001b[39m.\u001b[39mlong())  \u001b[39m# cross entropy loss\u001b[39;00m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# clear gradients for this training step\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# backpropagation, compute gradients\u001b[39;00m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# apply gradients\u001b[39;00m\n\u001b[0;32m     16\u001b[0m test_fea_norm1\u001b[39m=\u001b[39m test_fea_norm1\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = []\n",
    "best_auc = []\n",
    "\n",
    "# training and testing\n",
    "start_time = time.perf_counter()\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (train_x, train_y) in enumerate(train_loader):\n",
    "        \n",
    "        train_x= train_x.transpose(1,2)\n",
    "        output = lstm(train_x)  # LSTM output of training data\n",
    "        loss = loss_func(output, train_y.long())  # cross entropy loss\n",
    "        optimizer.zero_grad()  # clear gradients for this training step\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients\n",
    "\n",
    "        test_fea_norm1= test_fea_norm1.transpose(1,2)\n",
    "        \n",
    "        if epoch % 10 == 0 and step==2:\n",
    "            \n",
    "            test_output = lstm(test_fea_norm1)  # LSTM output of test data\n",
    "            test_loss = loss_func(test_output, test_label.long())\n",
    "\n",
    "            test_y_score = one_hot(test_label.data.cpu().numpy())  # .cpu() can be removed if your device is cpu.\n",
    "            pred_score = F.softmax(test_output, dim=1).data.cpu().numpy()  # normalize the output\n",
    "            auc_score = roc_auc_score(test_y_score, pred_score)\n",
    "\n",
    "            pred_y = torch.max(test_output, 1)[1].data.cpu().numpy()\n",
    "            pred_train = torch.max(output, 1)[1].data.cpu().numpy()\n",
    "\n",
    "            test_acc = accuracy_score(test_label.data.cpu().numpy(), pred_y)\n",
    "            train_acc = accuracy_score(train_y.data.cpu().numpy(), pred_train)\n",
    "\n",
    "\n",
    "            print('Epoch: ', epoch, '|train loss: %.4f' % loss.item(),\n",
    "                  ' train ACC: %.4f' % train_acc, '| test loss: %.4f' % test_loss.item(),\n",
    "                  'test ACC: %.4f' % test_acc, '| AUC: %.4f' % auc_score)\n",
    "            best_acc.append(test_acc)\n",
    "            best_auc.append(auc_score)\n",
    "\n",
    "current_time = time.perf_counter()\n",
    "running_time = current_time - start_time\n",
    "print(classification_report(test_label.data.cpu().numpy(), pred_y))\n",
    "print('BEST TEST ACC: {}, AUC: {}'.format(max(best_acc), max(best_auc)))\n",
    "print(\"Total Running Time: {} seconds\".format(round(running_time, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
